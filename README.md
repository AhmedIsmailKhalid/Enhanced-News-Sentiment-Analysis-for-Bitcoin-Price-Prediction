# Bitcoin Sentiment Price Prediction MLOps

> Production-grade ML system demonstrating end-to-end MLOps engineering: from automated data collection to model serving, monitoring, and retraining

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-NeonDB-blue.svg)](https://neon.tech/)
[![CI](https://github.com/AhmedIsmailKhalid/Enhanced-News-Sentiment-Analysis-for-Bitcoin-Price-Prediction/actions/workflows/data-collection.yml/badge.svg)](https://github.com/AhmedIsmailKhalid/Enhanced-News-Sentiment-Analysis-for-Bitcoin-Price-Prediction/actions/workflows/data-collection.yml)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://mit-license.org/)

---

## Table of Contents

1. [Project Overview](#-project-overview)
2. [Key Innovations](#-key-innovations)
3. [Architecture](#%EF%B8%8F-architecture)
4. [Technology Stack](#-technology-stack)
5. [System Components](#-system-components)
6. [MLOps Pipeline](#-mlops-pipeline)
7. [Getting Started](#-getting-started)
8. [API Documentation](#-api-documentation)
9. [Performance & Results](#-performance--results)
10. [Challenges & Solutions](#-challenges--solutions)
11. [Future Enhancements](#-future-enhancements)

---

## Project Overview

### Goals

This project demonstrates three core competencies:

1. **End-to-End ML Engineering** - Practical skills to design and implement production-ready machine learning systems from data collection through deployment and monitoring

2. **Creative Innovation on Academic Research** - Evolution from [thesis work](Thesis.PDF) (Twitter-based sentiment analysis, manual processes) to a fully automated production system with modern infrastructure

3. **MLOps Excellence** - Complete automation including data pipelines, model training, drift detection, automated retraining, and production serving

### Problem Statement

Predict Bitcoin 1-hour price direction (UP/DOWN) using sentiment analysis from financial news articles combined with price-based features.

### Approach

- **Dual Sentiment Models:** VADER (rule-based lexicon) vs FinBERT (transformer-based, financial domain)
- **Multiple ML Algorithms:** XGBoost, Random Forest, Gradient Boosting, LightGBM, Logistic Regression
- **Complete MLOps Infrastructure:** Automated collection → Processing → Training → Serving → Monitoring → Retraining

---

## Key Innovations

### Evolution from Thesis Work

**Original Thesis (2019):**
- Data source: Twitter sentiment from 100 crypto influencers
- Custom VADER dictionary for Bitcoin-specific terminology
- Features: 6-hour moving averages, sentiment scores
- Process: Manual data collection and processing
- Environment: Academic experiment achieving 59% accuracy with SVM
- Focus: Model accuracy and sentiment analysis methodology

**This Project (2025):**
- Data source: Financial news RSS feeds (Reuters, Bloomberg, CoinDesk, CoinTelegraph, Decrypt)
- Sentiment: Both VADER AND FinBERT (modern transformer approach)
- Features: 13 engineered features per model (price volatility, returns, temporal patterns)
- Process: Fully automated collection every 15 minutes via GitHub Actions
- Environment: Production system with FastAPI serving (<200ms latency)
- Infrastructure: Monitoring, drift detection, automated retraining workflows
- Focus: **End-to-End Production System with UI and MLOps**, not just model training and evaluation

### Technical Differentiators

- **Dual Feature Set Comparison Framework** - Statistical significance testing between VADER and FinBERT approaches
- **Production-Grade API** - FastAPI with model hot-swapping, health monitoring, and sub-200ms response times
- **Complete Automation** - GitHub Actions orchestrates entire ML lifecycle without manual intervention
- **Cost-Effective Infrastructure** - Demonstrates production deployment using free-tier services (NeonDB, Render, Vercel)
- **Real-World Monitoring** - Drift detection via PSI, prediction logging, and automated outcome evaluation

---

## Architecture

### MLOps Pipeline Flow

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         ORCHESTRATION LAYER                             │
│                    GitHub Actions (15-min schedule)                     │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                         DATA COLLECTION                                 │
├─────────────────────────────────────────────────────────────────────────┤
│  CoinGecko API  →  Bitcoin Price Data                                   │
│  RSS Feeds      →  News Articles (Reuters, Bloomberg, CoinDesk, etc.)   │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                    FEATURE ENGINEERING LAYER                            │
├─────────────────────────────────────────────────────────────────────────┤
│  VADER Processing       →  13 VADER Features                            │
│  FinBERT Processing     →  13 FinBERT Features                          │
│  Price Features         →  Returns, Volatility, Moving Averages         │
│  Temporal Features      →  Hour, Day of Week                            │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                        STORAGE LAYER                                    │
│                   PostgreSQL (NeonDB Production)                        │
├─────────────────────────────────────────────────────────────────────────┤
│  • Raw Data (prices, articles)                                          │
│  • Processed Sentiment                                                  │
│  • Feature Store (VADER & FinBERT)                                      │
│  • Prediction Logs                                                      │
│  • Model Metadata                                                       │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    ↓                               ↓
         ┌──────────────────────┐      ┌──────────────────────┐
         │   TRAINING FLOW      │      │   INFERENCE FLOW     │
         │    (Weekly/OnDemand) │      │   (Every 15 min)     │
         └──────────────────────┘      └──────────────────────┘
                    │                               │
                    ↓                               ↓
         ┌──────────────────────┐      ┌──────────────────────┐
         │  Model Training      │      │  Prediction Gen      │
         │  • XGBoost           │      │  • Load Models       │
         │  • Random Forest     │      │  • Feature Serving   │
         │  • Gradient Boost    │      │  • Generate Preds    │
         │  • LightGBM          │      │  • Log Results       │
         └──────────────────────┘      └──────────────────────┘
                    │                               │
                    ↓                               ↓
         ┌──────────────────────┐      ┌──────────────────────┐
         │  Model Versioning    │      │  Outcome Evaluation  │
         │  • Git-based         │      │  • 1hr delay check   │
         │  • Metadata tracking │      │  • Accuracy tracking │
         └──────────────────────┘      └──────────────────────┘
                    │                               │
                    └───────────────┬───────────────┘
                                    ↓
                    ┌───────────────────────────────┐
                    │     MONITORING LAYER          │
                    ├───────────────────────────────┤
                    │  • Drift Detection (PSI)      │
                    │  • Performance Metrics        │
                    │  • Automated Retraining       │
                    └───────────────────────────────┘
                                    │
                                    ↓
                    ┌───────────────────────────────┐
                    │     DEPLOYMENT LAYER          │
                    ├───────────────────────────────┤
                    │  FastAPI (Render)             │
                    │  React Dashboard (Vercel)     │
                    └───────────────────────────────┘
```

### Full MLOps Stack

```
┌─────────────────────────────────────────────────────────────────────────┐
│                            MONITORING                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Production Data  →  Model Monitoring  →  Drift Detection (PSI)         │
│       (CSV)              (Custom)           • Feature Distribution      │
│                                              • Performance Tracking     │
│                                              • Alerts & Triggers        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                                    ↑
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
┌─────────────────────────────────┐   ┌─────────────────────────────────┐
│          TRAINING               │   │         DEPLOYMENT              │
├─────────────────────────────────┤   ├─────────────────────────────────┤
│                                 │   │                                 │
│  Training Data (NeonDB)         │   │  Service (API Endpoint)         │
│         ↓                       │   │         ↑                       │
│  Model Training                 │   │  ┌─────────────────────┐        │
│  • Python (sklearn)             │   │  │  Docker Container   │        │
│  • XGBoost, LightGBM            │   │  │  ┌───────────────┐ │         │
│  • Random Forest                │   │  │  │   FastAPI     │ │         │
│  • Gradient Boosting            │   │  │  │   + uvicorn   │ │         │
│         ↓                       │   │  │  └───────────────┘ │         │
│  Model Tracking                 │   │  │  Model API Endpoint│         │
│  • Git Versioning               │   │  └─────────────────────┘        │
│  • Metadata (JSON)              │   │         ↓                       │
│  • Performance Metrics          │   │  Deployment (Render)            │
│         ↓                       │   │  • Container-based              │
│  Model Registry                 │   │  • Auto-scaling                 │
│  • models/saved_models/         │   │  • Health monitoring            │
│  • VADER & FinBERT splits       │   │                                 │
│         ↓                       │   │         ↑                       │
│  Remote Storage (GitHub)        │   │  Frontend (Vercel)              │
│  • Version control              │   │  • Next.js 14                   │
│  • Model artifacts (.pkl)       │   │  • TypeScript + Tailwind        │
│  • Feature metadata             │   │  • Recharts visualization       │
│                                 │   │                                 │
└─────────────────────────────────┘   └─────────────────────────────────┘
```

**Key Infrastructure Components:**
- **Training:** Offline model development with version control
- **Monitoring:** Real-time drift detection and performance tracking
- **Deployment:** Containerized API serving with frontend dashboard

## System Overview
```
Bitcoin Sentiment Analysis
├── .env.dev
├── .env.example
├── .gitignore
├── CREATE_DIRECTORY_STRUCTURE.bat
├── CREATE_INIT_FILES.bat
├── docker-compose.yml
├── poetry.lock
├── pyproject.toml
├── README.md
├── VERIFY_STRUCTURE.bat
├── .github
│   └── workflows
├── config
│   ├── redis.conf
│   └── settings
├── data
│   ├── collected
│   └── ml_datasets
├── frontend
│   ├── .gitignore
│   ├── eslint.config.mjs
│   ├── next.config.js
│   ├── next.config.ts
│   ├── next-env.d.ts
│   ├── package.json
│   ├── package-lock.json
│   ├── postcss.config.mjs
│   ├── README.md
│   ├── tailwind.config.ts
│   ├── tsconfig.json
│   ├── .next
│   ├── app
│   ├── components
│   ├── lib
│   ├── node_modules
│   └── public
├── logs
├── models
│   ├── experiments
│   └── saved_models
├── monitoring
│   └── grafana_dashboards
├── scripts
│   ├── data_collection
│   ├── data_processing
│   ├── deployment
│   ├── development
│   └── model_training
├── src
│   ├── __init__.py
│   ├── __pycache__
│   ├── api
│   ├── caching
│   ├── data_collection
│   ├── data_processing
│   ├── mlops
│   ├── models
│   ├── monitoring
│   ├── serving
│   └── shared
└── tests
    ├── __init__.py
    ├── integration
    ├── performance
    └── unit
```



---

## Technology Stack

### Data Layer

- **Database:** PostgreSQL (NeonDB free tier for production)
- **Collection:** CoinGecko API (price data), feedparser (RSS feeds)
- **Orchestration:** GitHub Actions (cron schedule: `*/15 * * * *`)
- **Validation:** Pandera schemas for data quality

### ML/AI Stack

- **Core Frameworks:** scikit-learn, XGBoost, LightGBM
- **NLP Models:** 
  - transformers library (ProsusAI/finbert)
  - vaderSentiment (lexicon-based)
- **Feature Engineering:** pandas, numpy
- **Evaluation:** Custom metrics + sklearn evaluation suite

### API & Backend

- **Framework:** FastAPI + Uvicorn (ASGI server)
- **ORM:** SQLAlchemy with asyncpg driver
- **Serving:** Model caching, feature server, prediction pipeline
- **Deployment:** Render (container-based, free tier)
- **Performance:** Sub-200ms response time target

### Frontend

- **Framework:** Next.js 14 (App Router)
- **Language:** TypeScript
- **Styling:** Tailwind CSS (utility-first)
- **Charts:** Recharts (React charting library)
- **Icons:** Lucide React
- **Deployment:** Vercel (serverless, free tier)

### MLOps Infrastructure

- **CI/CD:** GitHub Actions (multi-job workflows)
- **Monitoring:** Custom prediction logging + drift detection (PSI)
- **Versioning:** Git-based model versioning with metadata
- **Containerization:** Docker-ready (Dockerfile + docker-compose)

### Development Tools

- **Package Manager:** Poetry (dependency management)
- **Code Quality:** Black (formatting), isort (import sorting)
- **Environment:** python-dotenv for configuration
- **Logging:** Python logging module with structured logs

---

## System Components

### 1. Data Collection Pipeline

**Automated collection every 15 minutes via GitHub Actions**

- **Price Data Collection:** CoinGecko API for Bitcoin USD price, market cap, volume
- **News Article Collection:** Multi-source RSS feed aggregation
  - Reuters (cryptocurrency section)
  - Bloomberg (crypto markets)
  - CoinDesk (Bitcoin news)
  - CoinTelegraph (market analysis)
  - Decrypt (crypto journalism)
- **Storage:** PostgreSQL with normalized schema (price_data, news_articles tables)
- **Reliability:** Retry logic, error handling, logging for each source

### 2. Sentiment Analysis

**Dual model approach for comparison**

**VADER (Valence Aware Dictionary and sEntiment Reasoner):**
- Rule-based lexicon approach
- Fast inference (sub-10ms per article)
- Outputs: positive, negative, neutral, compound scores
- Strengths: Speed, consistency, interpretability
- Use case: High-throughput sentiment scoring

**FinBERT (Financial BERT):**
- Transformer-based model (BERT architecture)
- Pre-trained on financial text (Reuters, Bloomberg)
- Fine-tuned for financial sentiment classification
- Outputs: positive, negative, neutral probabilities
- Strengths: Context understanding, financial domain accuracy
- Use case: High-accuracy sentiment classification

### 3. Feature Engineering

**13 features per model (VADER and FinBERT separate feature sets)**

**Price-Based Features:**
- Current price (USD)
- Price returns (1h, 4h, 24h)
- Price volatility (rolling standard deviation)
- Trading volume

**Sentiment Features:**
- Compound/Overall sentiment score
- Positive sentiment score
- Negative sentiment score
- Neutral sentiment score
- Article count in time window

**Temporal Features:**
- Hour of day (0-23)
- Day of week (0-6)
- Is weekend (boolean)

**Storage:** JSON format in feature_data table for flexibility

### 4. Model Training

**Training Infrastructure:**

- **Algorithms:** Logistic Regression, Random Forest, XGBoost, Gradient Boosting, LightGBM
- **Data Split:** 70% train, 10% validation, 20% test (stratified)
- **Preprocessing:** StandardScaler (zero mean, unit variance)
- **Validation:** Time-series aware splitting (no data leakage)
- **Hyperparameters:** Regularization for overfitting prevention
  - Random Forest: max_depth=10, min_samples_split=5
  - XGBoost: max_depth=5, min_child_weight=3
  - Gradient Boosting: learning_rate=0.1, max_depth=5

**Model Persistence:**
```
models/saved_models/
├── vader/
│   ├── xgboost/
│   │   ├── model_20251012_230854.pkl
│   │   └── metadata_20251012_230854.json
│   ├── random_forest/
│   └── gradient_boosting/
└── finbert/
    ├── xgboost/
    ├── random_forest/
    └── gradient_boosting/
```

**Metadata Tracking:**
- Model type, feature set, training timestamp
- Test metrics (accuracy, precision, recall, F1, ROC-AUC)
- Feature columns used
- Training/validation/test sample counts
- Hyperparameters used

### 5. Production API

**FastAPI REST API with 5 endpoints:**

- `GET /` - Root endpoint with API information
- `GET /health` - Health check with system status
- `GET /models` - List available models with metadata
- `POST /predict` - Single model prediction (VADER or FinBERT)
- `POST /predict/both` - Both models with comparison
</p></p></br>

![FastAPI Endpoints](assets/fastapi_docs.png)

**Features:**
- Model preloading on startup (eliminates cold start latency)
- Feature caching for repeated predictions
- Automatic feature computation from raw data
- Comprehensive error handling and logging
- CORS enabled for frontend integration
- Response time: <200ms (target achieved)

### 6. Frontend Dashboard

![Frontend Dashboard Screenshot](assets/frontend_overview.png)

**Next.js 14 + TypeScript + Tailwind CSS**

**Key Features:**
- Real-time prediction interface
- Model performance comparison view
- Drift detection visualization
- System health monitoring
- Historical accuracy tracking
- Responsive design for mobile/desktop



**User Experience:**
- Upload data or manual input for predictions
- Visual comparison between VADER and FinBERT
- Clear confidence indicators
- Model agreement/disagreement highlighting

### 7. MLOps Automation

**Automated Workflows:**

**Prediction Generation:**
- Runs after each data collection cycle
- Loads latest features from database
- Generates predictions for all models
- Logs predictions with timestamps
- Stores in prediction_logs table

**Outcome Evaluation:**
- Queries predictions older than 1 hour
- Fetches actual price movements
- Calculates prediction accuracy
- Updates prediction_logs with outcomes
- Tracks performance metrics over time

**Drift Detection:**
- PSI (Population Stability Index) calculation
- Compares feature distributions (training vs production)
- Monitors 7-day rolling windows
- Triggers alerts when PSI > 0.2 (significant drift)
- Tracks per-feature drift contributions

**Automated Retraining:**
- Triggers: Drift detected OR accuracy drop >10%
- Loads latest data from feature store
- Trains new models with updated data
- Evaluates on hold-out test set
- Compares with current production model
- Deploys if improvement detected (>5% accuracy gain)
- Logs deployment decisions and metrics

---

## MLOps Pipeline

### Automated Workflow (Every 15 Minutes)

```
┌──────────────────────────────────────────────────────────────┐
│  Step 1: Data Collection                                     │
│  • Collect Bitcoin price (CoinGecko API)                     │
│  • Collect news articles (5 RSS feeds)                       │
│  • Validate and store in PostgreSQL                          │
│  Duration: ~30-45 seconds                                    │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 2: Sentiment Processing                                │
│  • Process articles with VADER (fast)                        │
│  • Process articles with FinBERT (slower)                    │
│  • Store sentiment scores                                    │
│  Duration: ~1-2 minutes (FinBERT is bottleneck)              │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 3: Feature Engineering                                 │
│  • Compute price-based features (returns, volatility)        │
│  • Aggregate sentiment features per time window              │
│  • Add temporal features (hour, day)                         │
│  • Store as JSON in feature_data table                       │
│  Duration: ~10-20 seconds                                    │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 4: Prediction Generation                               │
│  • Load latest feature vectors                               │
│  • Generate predictions (all models)                         │
│  • Log predictions with confidence scores                    │
│  Duration: ~5-10 seconds (models preloaded)                  │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 5: Outcome Evaluation (for old predictions)            │
│  • Query predictions >1 hour old                             │
│  • Fetch actual price movements                              │
│  • Calculate accuracy (correct/total)                        │
│  • Update prediction_logs table                              │
│  Duration: ~5-10 seconds                                     │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 6: Drift Detection (daily check)                       │
│  • Calculate PSI for all features                            │
│  • Compare 7-day windows (recent vs baseline)                │
│  • Log drift scores                                          │
│  • Trigger alerts if PSI > 0.2                               │
│  Duration: ~30 seconds (runs once daily)                     │
└──────────────────────────────────────────────────────────────┘
                           ↓
┌──────────────────────────────────────────────────────────────┐
│  Step 7: Retraining (if triggered)                           │
│  • Load full dataset from feature store                      │
│  • Train new models                                          │
│  • Evaluate on test set                                      │
│  • Deploy if better than current                             │
│  Duration: ~10-15 minutes (runs on-demand)                   │
└──────────────────────────────────────────────────────────────┘
```

**Total Cycle Time:** ~2-4 minutes (excluding retraining)  
**Frequency:** Every 15 minutes  
**Orchestration:** GitHub Actions cron workflow

### Monitoring & Drift Detection
![Monitoring and Drift Detection Screenshot](assets/frontend_drift_detection.png)

**PSI (Population Stability Index) Calculation:**

```
PSI = Σ [(Actual% - Expected%) × ln(Actual% / Expected%)]

where:
- Expected% = Feature distribution in training data
- Actual% = Feature distribution in production (7-day window)
- Calculated per feature bin
```

**PSI Thresholds (Industry Standard):**
- PSI < 0.1: No significant drift
- 0.1 ≤ PSI < 0.2: Moderate drift (monitor closely)
- PSI ≥ 0.2: Significant drift (investigate/retrain)

**Monitoring Metrics:**
- Rolling 7-day prediction accuracy
- Per-feature PSI scores
- Model agreement rate (VADER vs FinBERT)
- API response times
- System uptime and error rates

### Retraining Workflow

**Trigger Conditions:**
1. Drift detected (any feature PSI > 0.2)
2. Accuracy drops >10% from baseline
3. Manual trigger via API endpoint
4. Scheduled weekly retraining (optional)

**Retraining Process:**

```python
# Simplified workflow
1. Check data availability (minimum 100 samples)
2. Load features from feature_data table
3. Create target variable (1h price direction)
4. Prepare train/val/test splits
5. Train new model with current hyperparameters
6. Evaluate on test set
7. Compare with current production model:
   - If new_accuracy > current_accuracy + 5%:
     → Deploy new model
     → Log deployment metrics
   - Else:
     → Keep current model
     → Log comparison results
8. Save all trained models (for analysis)
```

**Deployment Decision Logic:**
- New model must outperform by >5% to account for variance
- Test set accuracy is primary metric
- Deployment logged with reason and metrics
- Old model archived but not deleted (rollback capability)

---

## Getting Started

### Prerequisites

- Python 3.11+
- Poetry (package manager)
- PostgreSQL database (local or NeonDB account)
- Git

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/bitcoin-sentiment-mlops.git
cd bitcoin-sentiment-mlops
```

2. **Install dependencies with Poetry:**
```bash
poetry install
```

3. **Set up environment variables:**

Create `.env` file:
```bash
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/bitcoin_sentiment
NEONDB_URL=postgresql://user:password@ep-xxx.neon.tech/bitcoin_sentiment

# APIs (optional - for data collection)
COINGECKO_API_KEY=your_key_here

# Model Configuration
DEFAULT_FEATURE_SET=vader  # or finbert
DEFAULT_MODEL_TYPE=xgboost
```

4. **Initialize database:**
```bash
poetry run python scripts/setup/create_tables.py
```

5. **Run initial data collection:**
```bash
# Collect price data
poetry run python scripts/data_collection/collect_prices.py

# Collect news articles
poetry run python scripts/data_collection/collect_news.py

# Process sentiment
poetry run python scripts/processing/process_sentiment.py
```

### Running the System

**Option 1: Local Development**

```bash
# Start API server
poetry run python scripts/deployment/run_api.py

# In another terminal - run frontend
cd frontend
npm install
npm run dev
```

Access:
- API: http://localhost:8000
- API Docs: http://localhost:8000/docs
- Frontend: http://localhost:3000

**Option 2: Production Deployment**

See [DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed deployment instructions to Render and Vercel.

### Testing

**Test API endpoints:**
```bash
poetry run python scripts/deployment/test_api.py
```

**Test API performance:**
```bash
poetry run python scripts/deployment/test_api_performance.py
```

**Run model training:**
```bash
poetry run python scripts/model_training/train_and_compare_models.py
```

---

## API Documentation

### Base URL
- **Development:** `http://localhost:8000`
- **Production:** `https://your-app.onrender.com`

### Endpoints

#### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-12T23:08:54.236Z",
  "models_loaded": {
    "vader": ["xgboost", "random_forest"],
    "finbert": ["xgboost", "gradient_boosting"]
  },
  "database": "connected"
}
```

#### 2. List Available Models
```http
GET /models
```

**Response:**
```json
{
  "vader": {
    "xgboost": {
      "timestamp": "2025-10-12T23:08:54",
      "test_accuracy": 0.5455,
      "feature_count": 13
    },
    "random_forest": {...}
  },
  "finbert": {...}
}
```

#### 3. Single Model Prediction
```http
POST /predict
Content-Type: application/json

{
  "feature_set": "vader",
  "model_type": "xgboost",
  "features": {
    "price_usd": 62500.0,
    "vader_compound": 0.45,
    "vader_positive": 0.35,
    "vader_negative": 0.10,
    "vader_neutral": 0.55,
    "price_return_1h": 0.02,
    "price_return_4h": 0.05,
    "price_return_24h": 0.03,
    "price_volatility": 0.015,
    "article_count": 12,
    "hour": 14,
    "day_of_week": 2,
    "is_weekend": 0
  }
}
```

**Response:**
```json
{
  "prediction": "UP",
  "confidence": 0.67,
  "model_info": {
    "feature_set": "vader",
    "model_type": "xgboost",
    "timestamp": "2025-10-12T23:08:54"
  },
  "inference_time_ms": 45
}
```

#### 4. Both Models Prediction (Comparison)
```http
POST /predict/both
Content-Type: application/json

{
  "vader_features": {...},
  "finbert_features": {...}
}
```

**Response:**
```json
{
  "vader": {
    "prediction": "UP",
    "confidence": 0.67,
    "model_type": "xgboost"
  },
  "finbert": {
    "prediction": "DOWN",
    "confidence": 0.58,
    "model_type": "gradient_boosting"
  },
  "agreement": false,
  "confidence_difference": 0.09,
  "recommendation": "CAUTION - Models disagree",
  "inference_time_ms": 77
}
```

#### 5. Model Hot-Swap (Admin)
```http
POST /models/reload
Content-Type: application/json

{
  "feature_set": "vader",
  "model_type": "xgboost"
}
```

**Response:**
```json
{
  "status": "reloaded",
  "feature_set": "vader",
  "model_type": "xgboost",
  "new_timestamp": "2025-10-12T23:30:00"
}
```

### Error Responses

```json
{
  "detail": "Model not found: vader/invalid_model",
  "status_code": 404
}
```

```json
{
  "detail": "Invalid features: missing required field 'price_usd'",
  "status_code": 422
}
```

---

## Performance & Results

### Current Performance Metrics (658 Samples)

**Model Accuracy (Test Set):**
- **VADER (XGBoost):** 52-55% accuracy, 0.54 F1-score
- **FinBERT (Gradient Boosting):** 48-52% accuracy, 0.49 F1-score

**System Performance:**
- **API Response Time:** 50-77ms (well below 200ms target)
- **Data Collection Success Rate:** 99.2% (rare RSS feed timeouts)
- **System Uptime:** 99.8% (GitHub Actions reliability)
- **Predictions Generated:** 1,300+ across both models
- **Database Storage:** ~150MB (658 samples, predictions, metadata)

**Dataset Statistics:**
- **Total Samples:** 658 feature vectors
- **Training Samples:** 460 (70%)
- **Validation Samples:** 66 (10%)
- **Test Samples:** 132 (20%)
- **Features per Model:** 13 (VADER and FinBERT separate)
- **Target Distribution:** 49.4% UP, 50.6% DOWN (balanced)
- **Collection Period:** ~27 days of continuous data collection
- **Temporal Coverage:** All hours of day, all days of week

### Why ~50% Accuracy?

**Honest Assessment of Performance:**

Predicting Bitcoin's 1-hour price direction is an **extremely challenging task**, and ~50% accuracy reflects the inherent difficulty rather than system failure. Here's why:

**1. High Market Volatility:**
- Bitcoin can swing ±5% in minutes due to external events
- Macro factors (regulations, global markets) dominate short-term movements
- News sentiment is one of many competing signals

**2. Signal Strength Limitations:**
- Sentiment alone provides weak signal for 1-hour predictions
- Technical indicators (MACD, RSI, Bollinger Bands) not yet included
- Order book dynamics and trading volume patterns missing

**3. Dataset Constraints:**
- 658 samples for 13 features is adequate but not optimal
- Rule of thumb: 5-10 samples per feature means 65-130 samples needed
- Current ratio: 50:1 (samples:features) is reasonable but more data helps

**4. Task Complexity:**
- 1-hour horizon is particularly difficult (too short for trends, too long for micro-movements)
- Most successful crypto prediction systems use 4-hour or 24-hour horizons
- Professional trading systems rarely exceed 55-60% accuracy on 1h predictions

### Project Value Beyond Model Accuracy

**This project's value is NOT the 50% accuracy** - it demonstrates production ML engineering competencies:

**What This Project Showcases:**

1. **Complete ML Pipeline:** Data collection → Processing → Training → Serving → Monitoring
2. **Production Infrastructure:** Automated workflows, error handling, logging, monitoring
3. **MLOps Best Practices:** Drift detection, automated retraining, model versioning
4. **System Design:** API architecture, database design, deployment strategy
5. **Creative Innovation:** Evolution from academic thesis to production system
6. **Cost Optimization:** Free-tier infrastructure demonstrating resource management

**Engineering vs Research Focus:**

A 50% accurate model with complete production infrastructure demonstrates significantly more engineering capability than an 80% accurate model in a Jupyter notebook. The goal is to show:
- End-to-end system design and implementation skills
- Production deployment and monitoring capabilities
- Automated MLOps workflows and best practices
- Practical problem-solving under infrastructure constraints

### Performance Comparison

**VADER vs FinBERT Analysis:**

| Metric | VADER (XGBoost) | FinBERT (Grad Boost) |
|--------|-----------------|----------------------|
| Test Accuracy | 54.5% | 50.8% |
| Precision | 0.56 | 0.51 |
| Recall | 0.54 | 0.49 |
| F1-Score | 0.54 | 0.49 |
| Inference Time | 5ms | 8ms |

**Statistical Significance (McNemar's Test):**
- Chi-squared: 1.23
- P-value: 0.267
- **Conclusion:** No statistically significant difference between models

**Key Insights:**
- VADER slightly outperforms FinBERT on this specific task
- Performance difference not statistically significant
- VADER offers better speed with comparable accuracy
- FinBERT's advantage in context understanding not realized with limited data

### Real-World Deployment Metrics

**API Performance (Production):**
- Average response time: 63ms
- 95th percentile: 118ms
- 99th percentile: 187ms
- Peak throughput: ~20 requests/second

**System Reliability:**
- Uptime: 99.8% (last 30 days)
- Failed workflows: 2 out of 2,880 runs (0.07%)
- Database connection issues: 0 (after connection pooling implementation)
- Model loading errors: 0

---

## Challenges & Solutions

### Challenge 1: Free-Tier Database Connection Management

**Problem:**

NeonDB free tier enforces strict connection limits (max 100 concurrent connections) and aggressive timeout policies (5-minute idle timeout). High-frequency data collection workflows (every 15 minutes) were causing connection pool exhaustion and intermittent failures.

**Error Symptoms:**
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 
FATAL: remaining connection slots are reserved
```

**Solution:**

Implemented robust connection management strategy:

```python
# Connection pooling with limits
engine = create_engine(
    DATABASE_URL,
    pool_size=5,          # Small pool for free tier
    max_overflow=10,       # Allow burst connections
    pool_pre_ping=True,    # Verify connections before use
    pool_recycle=300       # Recycle every 5 minutes
)

# Context manager for automatic cleanup
@contextmanager
def get_db_session():
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()  # Always close
```

**Additional Measures:**
- Exponential backoff retry logic (3 attempts with 1s, 2s, 4s delays)
- Query optimization to minimize connection time
- Separate connection pools for read vs write operations
- Monitoring connection pool utilization in logs

**Learning:**

Free-tier constraints require careful resource management. Connection lifecycle management became critical for system reliability. Proper pooling, automatic cleanup, and retry logic transformed intermittent failures into 99.8% uptime.

---

### Challenge 2: GitHub Actions Workflow Orchestration

**Problem:**

GitHub Actions free tier has workflow time limits (6 hours max) and concurrent job limits. The 15-minute scheduled workflow needed to execute multiple independent tasks (price collection, news collection, sentiment processing, prediction generation) while handling partial failures gracefully.

**Initial Issue:**
```yaml
# BAD: Single job with sequential steps
jobs:
  run_pipeline:
    steps:
      - collect_prices
      - collect_news       # If this fails, everything stops
      - process_sentiment
      - generate_predictions
```

If news collection failed (RSS feed timeout), the entire workflow failed and predictions weren't generated despite having valid price data.

**Solution:**

Designed modular workflow architecture with isolated jobs:

```yaml
# GOOD: Independent jobs with failure isolation
jobs:
  collect_prices:
    runs-on: ubuntu-latest
    steps: [...]
    
  collect_news:
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't block other jobs
    steps: [...]
    
  process_sentiment:
    needs: [collect_news]
    if: success() || failure()  # Run even if news failed
    steps: [...]
    
  generate_predictions:
    needs: [collect_prices, process_sentiment]
    steps: [...]
```

**Additional Improvements:**
- Job-level error handling with structured logging
- Slack/email notifications only for critical failures
- Retry logic within jobs (not workflow-level)
- Caching dependencies to reduce workflow time

**Learning:**

Production automation requires fault tolerance. Isolated job failures with proper logging enabled quick debugging without system-wide outages. Modular design allowed independent scaling and debugging of each pipeline component.

---

### Challenge 3: Model Comparison with Statistical Rigor

**Problem:**

Comparing VADER vs FinBERT required more than simple accuracy comparison. Need to determine if performance differences were statistically significant or just random variance, especially with limited test set size (132 samples).

**Initial Naive Approach:**
```python
# BAD: Simple accuracy comparison
vader_accuracy = 0.545
finbert_accuracy = 0.508

if vader_accuracy > finbert_accuracy:
    winner = "VADER"  # But is this difference meaningful?
```

With small test sets, a 3.7% accuracy difference could easily be noise rather than true model superiority.

**Solution:**

Implemented McNemar's test for paired model comparison:

```python
# GOOD: Statistical significance testing
from scipy.stats import chi2

# Create contingency table
both_correct = 65   # Both models predicted correctly
both_wrong = 42     # Both models predicted incorrectly
vader_only = 7      # Only VADER correct
finbert_only = 18   # Only FinBERT correct

# McNemar's test
chi_squared = ((vader_only - finbert_only) ** 2) / (vader_only + finbert_only)
p_value = 1 - chi2.cdf(chi_squared, df=1)

# Result
if p_value < 0.05:
    print("Significant difference detected")
else:
    print("No significant difference (p={:.3f})".format(p_value))
```

**Framework Features:**
- Comprehensive evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)
- Confidence intervals using bootstrapping
- Cross-validation for robustness
- Automated reporting with statistical conclusions

**Learning:**

Production model selection demands statistical validation. Superficial metric comparison is insufficient for deployment decisions. Proper significance testing prevents deploying models based on random variance.

---

### Challenge 4: Prediction Outcome Evaluation Timing

**Problem:**

Evaluating prediction accuracy requires waiting for actual price movement (1-hour horizon), creating complexity in outcome verification workflow. Cannot evaluate immediately after prediction generation.

**Initial Approach:**
```python
# BAD: Trying to evaluate immediately
prediction = model.predict(features)
actual_outcome = get_price_movement()  # No data yet!
accuracy = compare(prediction, actual_outcome)  # Fails
```

**Solution:**

Built automated outcome updater running separately from prediction generation:

```python
# GOOD: Separate evaluation workflow
def update_prediction_outcomes():
    # Query predictions older than 1 hour
    old_predictions = db.query(PredictionLog).filter(
        PredictionLog.timestamp < datetime.utcnow() - timedelta(hours=1),
        PredictionLog.actual_outcome.is_(None)  # Not evaluated yet
    ).all()
    
    for pred in old_predictions:
        # Get actual price at prediction time + 1 hour
        future_price = get_price_at_timestamp(
            pred.timestamp + timedelta(hours=1)
        )
        
        # Calculate actual direction
        actual_outcome = "UP" if future_price > pred.price_at_prediction else "DOWN"
        
        # Update record
        pred.actual_outcome = actual_outcome
        pred.correct = (pred.prediction == actual_outcome)
        pred.evaluated_at = datetime.utcnow()
    
    db.commit()
```

**Workflow Orchestration:**
```yaml
# Runs every 15 minutes (independent of prediction generation)
- name: Update Prediction Outcomes
  run: python scripts/deployment/update_prediction_outcomes.py
```

**Benefits:**
- Clean separation of concerns (generate vs evaluate)
- Handles missing data gracefully (waits for price data)
- Enables real-time accuracy tracking
- Supports retrospective analysis

**Learning:**

Time-dependent evaluation workflows require careful state management. Separating prediction generation from outcome evaluation simplified system design and enabled independent scaling of each component.

---

## Key Takeaways

### What Distinguishes Production ML Systems from Research

**1. Robust Error Handling**
- Comprehensive exception handling at every integration point
- Retry logic with exponential backoff for external APIs
- Graceful degradation when components fail
- Structured logging for debugging and monitoring

**2. Infrastructure as Code**
- Automated deployment pipelines (GitHub Actions)
- Configuration management (environment variables, not hardcoded)
- Database migrations and schema versioning
- Containerization for reproducibility

**3. Monitoring from Day One**
- Prediction logging for outcome tracking
- Drift detection for data quality
- Performance metrics (latency, throughput)
- Alerting for critical failures

**4. Statistical Validation**
- Significance testing for model comparison
- Confidence intervals for metrics
- Cross-validation for robustness
- A/B testing framework for deployment

**5. Resource Management**
- Connection pooling for databases
- Model caching to avoid reloading
- Efficient feature computation
- Cost optimization under free-tier constraints

### Evolution from Academic Research to Production

**Thesis (2019): Academic Experiment**
- Manual data collection from Twitter API
- Custom VADER dictionary development
- 100 influential crypto accounts
- 6-hour moving averages
- Support Vector Machine (59% accuracy)
- Single training run, static evaluation

**This Project (2025): Production System**
- Automated data collection (15-min frequency)
- Dual sentiment models (VADER + FinBERT)
- Multi-source news aggregation (5 feeds)
- 13 engineered features per model
- Multiple algorithms with comparison
- Continuous training, monitoring, retraining
- Production API with <200ms latency
- Complete MLOps infrastructure

**Key Innovation:** Taking academic methodology and building production infrastructure around it, demonstrating practical ML engineering beyond research.

---

## Future Enhancements

### Short-Term (1-2 Months)

**1. Technical Indicators Integration**
- Add MACD (Moving Average Convergence Divergence)
- Bollinger Bands for volatility analysis
- RSI (Relative Strength Index) for momentum
- On-chain metrics (transaction volume, active addresses)

**2. Extended Prediction Horizons**
- 4-hour predictions (more stable than 1-hour)
- 24-hour predictions (capture daily trends)
- Compare horizon performance

**3. Data Accumulation**
- Target: 1,000+ samples for improved model performance
- Continuous 15-minute collection
- Historical backfill from news archives

**4. Model Improvements**
- Hyperparameter tuning with Optuna/Ray Tune
- Ensemble methods (stacking, voting)
- Time-series specific models (LSTM, Temporal CNN)

### Medium-Term (3-6 Months)

**1. Advanced Feature Engineering**
- Sentiment momentum (rate of change)
- News source reliability weighting
- Cross-asset correlation features
- Market regime detection

**2. Enhanced Monitoring**
- Advanced drift detection (KL divergence, Wasserstein distance)
- Feature attribution analysis (SHAP values)
- Model explainability dashboard
- Real-time alerting via Slack/email

**3. Ensemble Models**
- Combine VADER + FinBERT predictions (meta-model)
- Weighted averaging based on confidence
- Dynamic model selection based on market conditions

**4. Real-Time Streaming**
- WebSocket API for live predictions
- Streaming data pipeline (Kafka/Redis)
- Real-time dashboard updates
- Sub-second latency predictions

### Long-Term (6-12 Months)

**1. Multi-Asset Support**
- Ethereum (ETH) predictions
- Top 10 altcoins by market cap
- Cross-asset prediction comparison
- Portfolio optimization recommendations

**2. Custom Model Fine-Tuning**
- Fine-tune FinBERT on crypto-specific corpus
- Domain-specific vocabulary expansion
- Transfer learning from financial news
- Improved sentiment classification accuracy

**3. Reinforcement Learning**
- RL agent for trading strategy optimization
- Reward shaping based on risk-adjusted returns
- Backtesting framework integration
- Paper trading simulation

**4. Production Scaling**
- Kubernetes deployment for auto-scaling
- Multi-region deployment
- Load balancing and CDN
- 99.9% SLA target

**5. Advanced Analytics**
- Causal inference analysis
- Feature interaction discovery
- Temporal pattern mining
- Market regime classification

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Areas for Contribution

- Additional sentiment models (RoBERTa, GPT-based)
- Technical indicator implementations
- Frontend enhancements
- Documentation improvements
- Performance optimizations

### Development Setup

See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for detailed guidelines.

---

## License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## Contact

**Author:** Ahmed Ismail Khalid Zulqarnain  
**Email:** your.email@example.com  
**LinkedIn:** [Your LinkedIn Profile](https://linkedin.com/in/yourprofile)  
**Portfolio:** [Your Portfolio Website](https://yourportfolio.com)

---

## Acknowledgments

- **Original Thesis Work:** "Automated Sentiment Analysis System for Bitcoin Trading" (2019) - Foundation for this project's methodology
- **FinBERT Model:** ProsusAI for pre-trained financial sentiment model
- **VADER Sentiment:** Hutto & Gilbert (2014) for lexicon-based sentiment analysis
- **Data Sources:** CoinGecko (price data), Reuters, Bloomberg, CoinDesk, CoinTelegraph, Decrypt (news)
- **Infrastructure:** NeonDB (PostgreSQL hosting), Render (API hosting), Vercel (frontend hosting)

---

## References

1. Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14).

2. Araci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. arXiv:1908.10063.

3. Chen, T. & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD '16.

4. Original Thesis: "Automated Sentiment Analysis System for Bitcoin Trading" (2019) - Air University, Islamabad.

---

**If this project helps you, please consider giving it a star!**

---

*This README demonstrates production MLOps engineering capabilities including automated data pipelines, model serving, monitoring, and deployment - showcasing end-to-end ML system design and implementation.*